{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rede-neural-do zero.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM7qT1F2pTFkD3TpZxxqShy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PRJacovac/Machine-Learning-Specialist/blob/main/Rede_neural_do_zero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BP8MgECYFTW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor() #definindo a conversão de imagem para tensor\n",
        "\n",
        "trainset = datasets.MNIST('./MNIST_data/', download=True, train=True, transform=transform) #carrega a parte de treino do dataset\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) #cria um buffer para pegar os dados por partes\n",
        "\n",
        "valset = datasets.MNIST('./MNIST_data', download=True, train=False, transform=transform) #Carrega a parte de validação do dataset\n",
        "valloader  =torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True) #Cria um buffer para pegar os dados por partes"
      ],
      "metadata": {
        "id": "9XiISZpuZpdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(trainloader)\n",
        "imagens, etiquetas = dataiter.next()\n",
        "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "9vUOJkqje3zL",
        "outputId": "574d9bd7-b264-49e6-b9c7-0a09d42a652a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(imagens[0].shape) #para verificar as dimensões do tensor de cada imagem\n",
        "print(etiquetas[0].shape) #verificar a dimensão do tensor de cada etiqueta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3tZpgkifn-7",
        "outputId": "dae9c24c-07dc-413d-f8ed-63384b40543c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Modelo(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Modelo, self).__init__()\n",
        "    self.linear1 = nn.Linear(28*28, 128) # camada de entrada, 784 neuronios que se ligam a 128\n",
        "    self.linear2 = nn.Linear(128, 64) # camada interna 1, 128 neuronios que se ligam a 64\n",
        "    self.linear3 = nn.Linear(64, 10) # camada interna 2, 64 neuronios que se ligam a 10\n",
        "    # para a camada de saida não é necessário definir nada pois só precisamos pegar o outpu da camada interna 2\n",
        "\n",
        "  def forward(self,X):\n",
        "    X = F.relu(self.linear(X)) # função de ativação da camada de entrada para a camada interna 1\n",
        "    X = F.relu(self.linear2(X)) # função de ativação da camada interna 1 para a camada interna 2\n",
        "    X = self.linear3(X) # função de ativação da camada interna 2 para a camada de saida, nesse caso f(x) = x\n",
        "    return F.log_softmax(X, dim=1) # dados utilizados para calcular a perda\n"
      ],
      "metadata": {
        "id": "WRKJD7qKhQ9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def treino(modelo, trainloader, device):\n",
        "\n",
        "  otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5) # define a politica de atualização dos pesos e da bias\n",
        "  inicio = time() # timer para sabermos quanto tempo levou o treino\n",
        "\n",
        "  criterio = nn.NLLLoss() # definindo o criterio para calcular a perda\n",
        "  EPOCHS = 10 # numero de epochs que o algoritmo rodará\n",
        "  modelo.train() # ativando o modo de treinamento do modelo\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    perda_acumulada = 0 # inicialização da perda acumulada da epoch em questão\n",
        "\n",
        "    for imagens, etiquetas in trainloader:\n",
        "\n",
        "      imagens = imagens.view(imagens.shape[0], -1) # convertendo as imagens para \"vetores\" de 28*28 casas para ficarem compativeis coma \n",
        "      otimizador.zero_grad() # zerandoos gradientes por conta do ciclo anterior\n",
        "\n",
        "      output = modelo(imagens.to(device)) # colocando os dados no modelo\n",
        "      perda_instantanea = criterio(output, etiquetas.to(device)) # calaculando a perda da epoch em questão\n",
        "\n",
        "      perda_instantanea.backward() # back propagation a partir da perda\n",
        "\n",
        "      otimizador.step() # atualizando os pesos e a bias\n",
        "\n",
        "      perda_acumulada += perda_instantanea.item() # atualização da perda acumulada\n",
        "\n",
        "  else:\n",
        "      print(\"Epoch {} - Perda resultante {}\".format(epoch+1, perda_acumulada/len(trainloader)))\n",
        "  print(\"\\nTempo de treino (em minutos) =\", (time()-inicio)/60)  \n"
      ],
      "metadata": {
        "id": "NnWTdeq2WhhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validacao(modelo, valloader, device):\n",
        "  conta_corretas, conta_todas = 0, 0\n",
        "  for imagens,etiquetas in valloader:\n",
        "    for i in range(len(etiquetas)):\n",
        "      img = imagens[i].view(1,784)\n",
        "      # desativar o autograd para acelerar a validacao. Grafos computacionais dinamicos tem um custo alto de processamento\n",
        "      with torch.no_grad():\n",
        "        logps = modelo(img.to(device)) # output do modelo em escala logaritmica\n",
        "\n",
        "      ps = torch.exp(logps) # converte output para escala normal (lembrando que é um tensor)\n",
        "      probab = list(ps.cpu().numpy()[0])\n",
        "      etiqueta_pred = probab.index(max(probab)) # converte o tensor em um numero, no caso, o numeroque o modelo previu como correto\n",
        "      etiqueta_certa = etiquetas.numpy()[i]\n",
        "      if(etiqueta_certa == etiqueta_pred): # compara a previsão com o valor correto\n",
        "        conta_corretas += 1\n",
        "      conta_todas += 1\n",
        "  print(\"Total de imagens testadas =\", conta_todas)\n",
        "  print(\"\\nPrecisão do modelo = {}%\".format(conta_corretas*100/conta_todas))"
      ],
      "metadata": {
        "id": "McDJdDq2ef2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = Modelo()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelo.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aigZ2vfJi8o4",
        "outputId": "7418e45f-04aa-4411-e0b3-7034c09b19be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Modelo(\n",
              "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}